{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc310dc-a4ca-4bee-8c46-6478c7cfd4dc",
   "metadata": {},
   "source": [
    "## Quantization-Aware Basecalling Neural Architecture Search (QABAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac566b66-3f70-47a4-84e7-36a246308c30",
   "metadata": {},
   "source": [
    "We use neural architecture search (NAS) to explore different design options for a basecaller. We use a differentiable NAS (DNAS) approach, a weight-sharing approach where we train only one supernetwork and distill a sub-network out of it. We define a search space that consists of all the options for a model. The search space for rubicon is defined in arch/basemodelquant.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "015b9390-4cd9-4198-89c7-c41778e381bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from argparse import ArgumentParser \n",
    "from argparse import ArgumentDefaultsHelpFormatter\n",
    "from pathlib import Path\n",
    "from importlib import import_module\n",
    "import torch.nn as nn\n",
    "from os import system\n",
    "from bonito.data import load_numpy\n",
    "from rubicon.data import load_numpy_shuf,load_numpy_full\n",
    "from bonito.data import load_script\n",
    "from rubicon.util import __models__, default_data\n",
    "from bonito.util import load_symbol, init\n",
    "from rubicon.training import load_state, Trainer\n",
    "import json\n",
    "import toml\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from rubicon.tools.nni.nni.retiarii.nn.pytorch.api import LayerChoice, InputChoice\n",
    "from rubicon.nas.dartsbasecalling import DartsBasecalling\n",
    "from rubicon.nas.proxylessbasecalling import ProxylessBasecalling\n",
    "import torch.onnx\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "sys.setrecursionlimit(20000)\n",
    "\n",
    "_logger = logging.getLogger(__name__)\n",
    "def get_parameters(model, keys=None, mode='include'):\n",
    "    if keys is None:\n",
    "        for name, param in model.named_parameters():\n",
    "            yield param\n",
    "    elif mode == 'include':\n",
    "        for name, param in model.named_parameters():\n",
    "            flag = False\n",
    "            for key in keys:\n",
    "                if key in name:\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                yield param\n",
    "    elif mode == 'exclude':\n",
    "        for name, param in model.named_parameters():\n",
    "            flag = True\n",
    "            for key in keys:\n",
    "                if key in name:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                yield param\n",
    "    else:\n",
    "        raise ValueError('do not support: %s' % mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6e8996-1444-481f-aad6-53190a9ec635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define all the parameters\n",
    "save_directory=\"temp\"\n",
    "workdir = os.path.expanduser(save_directory)\n",
    "seed=25\n",
    "config=\"../rubicon/models/configs/config.toml\"\n",
    "hardware='aie_lut'\n",
    "nas='proxy'\n",
    "reference_latency=\"65\"\n",
    "grad_reg_loss_lambda=6e-1\n",
    "directory=\"../rubicon/data/dna_r9.4.1\"\n",
    "lr=2e-3\n",
    "ctlr=2e-3\n",
    "grad_reg_loss_type=\"add#linear\"\n",
    "rubicon=True\n",
    "default=True\n",
    "epochs=5\n",
    "rub_sched=True\n",
    "dart_sched=True\n",
    "rub_arch_opt=True\n",
    "prox_arch_opt=True\n",
    "full=False\n",
    "chunks=128\n",
    "valid_chunks=128\n",
    "device=\"CPU\"\n",
    "arc_checkpoint=\"final_arch.json\"\n",
    "# !which python\n",
    "# assert(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11239600-8dab-45f8-b3d4-824524f5ef14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:50 AM [INFO] Start date and time:2023-11-07 06:55:50.655266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:50] INFO (__main__/MainThread) Start date and time:2023-11-07 06:55:50.655266\n",
      "[2023-11-07 06:55:50] INFO (__main__/MainThread) Start date and time:2023-11-07 06:55:50.655266\n",
      "[error] temp exists. Removing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:50 AM [INFO] [loading model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:50] INFO (__main__/MainThread) [loading model]\n",
      "[2023-11-07 06:55:50] INFO (__main__/MainThread) [loading model]\n",
      "BaseModelQuant model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] NAS type:proxy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) NAS type:proxy\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) NAS type:proxy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] Hardware type:aie_lut\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Hardware type:aie_lut\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Hardware type:aie_lut\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] Reference latency:65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Reference latency:65\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Reference latency:65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] lambda:0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) lambda:0.6\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) lambda:0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] [loading data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) [loading data]\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) [loading data]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] Not full dataset training with shuffling\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Not full dataset training with shuffling\n",
      "[2023-11-07 06:55:56] INFO (__main__/MainThread) Not full dataset training with shuffling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11/07/2023 06:55:56 AM [INFO] Dataset length: 128/1221470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-07 06:55:56] INFO (rubicon.data/MainThread) Dataset length: 128/1221470\n",
      "[2023-11-07 06:55:56] INFO (rubicon.data/MainThread) Dataset length: 128/1221470\n"
     ]
    }
   ],
   "source": [
    "_logger.info(\"Start date and time:{}\".format(datetime.datetime.now()))\n",
    "if os.path.exists(workdir):\n",
    "    print(\"[error] %s exists. Removing.\" % workdir)\n",
    "    os.rmdir(workdir)\n",
    "    exit(1)\n",
    "\n",
    "os.makedirs(workdir, exist_ok=True)\n",
    "# init(seed, device)\n",
    "# device = torch.device(device)\n",
    "\n",
    "\n",
    "config_file = config\n",
    "if not os.path.exists(config_file):\n",
    "    print(\"[error] %s does not\" % config_file)\n",
    "    exit(1)\n",
    "config = toml.load(config_file)\n",
    "if not nas:\n",
    "    _logger.warning(\"Please specify which type of NAS using --nas argument\")\n",
    "    exit(1)\n",
    "_logger.info(\"[loading model]\")\n",
    "model = load_symbol(config, 'BaseModelQuant')(config)\n",
    "\n",
    "\n",
    "_logger.info(\"NAS type:{}\".format(nas))\n",
    "_logger.info(\"Hardware type:{}\".format(hardware))\n",
    "_logger.info(\"Reference latency:{}\".format(reference_latency))\n",
    "_logger.info(\"lambda:{}\".format(grad_reg_loss_lambda))\n",
    "_logger.info(\"[loading data]\")\n",
    "if full:\n",
    "        _logger.info(\"Full dataset training\")\n",
    "        train_loader_kwargs, valid_loader_kwargs = load_numpy_full(None,\n",
    "                args.directory\n",
    "        )\n",
    "elif chunks:\n",
    "        _logger.info(\"Not full dataset training with shuffling\")\n",
    "        train_loader_kwargs, valid_loader_kwargs = load_numpy_shuf(\n",
    "            chunks, valid_chunks, directory\n",
    "        )\n",
    "else:\n",
    "        _logger.warning(\"Please define the training data correctly\")\n",
    "        exit(1)\n",
    "\n",
    "loader_kwargs = {\n",
    "    \"batch_size\": args.batch, \"num_workers\": 8, \"pin_memory\": True\n",
    "}\n",
    "train_loader = DataLoader(**loader_kwargs, **train_loader_kwargs)\n",
    "valid_loader = DataLoader(**loader_kwargs, **valid_loader_kwargs)\n",
    "\n",
    "if nas == 'darts':\n",
    "    #### setting optimizer #######  \n",
    "    optimizer = None\n",
    "    _logger.info(\"Starting DARTS NAS\")\n",
    "    #### setting lr scheduler #######\n",
    "\n",
    "    _logger.info(\"Scheduler: Linear Warmup\")\n",
    "    if config.get(\"lr_scheduler\"):\n",
    "        sched_config = config[\"lr_scheduler\"]\n",
    "        lr_scheduler_fn = getattr(\n",
    "            import_module(sched_config[\"package\"]), sched_config[\"symbol\"]\n",
    "        )(**sched_config)\n",
    "        print(\"building scheduler\",getattr(\n",
    "            import_module(sched_config[\"package\"]), sched_config[\"symbol\"]\n",
    "        )(**sched_config))\n",
    "    else:\n",
    "        print(\"no scheduler\")\n",
    "        lr_scheduler_fn = None\n",
    "\n",
    "\n",
    "\n",
    "    trainer = DartsBasecalling(\n",
    "            model=model,\n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler_fn=lr_scheduler_fn,\n",
    "            ctrl_learning_rate=ctlr,\n",
    "            opt_learning_rate=lr,\n",
    "            applied_hardware=hardware,\n",
    "            metrics=lambda output, target: accuracy(output, target, topk=(1, 5,)),\n",
    "            log_frequency=10,\n",
    "            grad_reg_loss_type=grad_reg_loss_type, \n",
    "            grad_reg_loss_params=grad_reg_loss_params, \n",
    "            dummy_input=(344,1,9),\n",
    "            ref_latency=reference_latency,\n",
    "            rubicon=rubicon,\n",
    "            default=default,\n",
    "            num_epochs=epochs\n",
    "        )\n",
    "\n",
    "elif nas == 'proxy':\n",
    "    #### setting optimizer STEP 2 UPDATE WEIGHTS #######\n",
    "    optimizer = None\n",
    "    _logger.info(\"Starting ProxylessNAS\")\n",
    "    lr_scheduler_fn=None  \n",
    "\n",
    "    trainer = ProxylessBasecalling(\n",
    "            model=model,\n",
    "            train_loader=train_loader, \n",
    "            valid_loader=valid_loader,\n",
    "            optimizer=optimizer,\n",
    "            lr_scheduler_fn=lr_scheduler_fn,\n",
    "            ctrl_learning_rate=ctlr,\n",
    "            applied_hardware=hardware,\n",
    "            metrics=lambda output, target: accuracy(output, target, topk=(1, 5,)),\n",
    "            log_frequency=10,\n",
    "            grad_reg_loss_type=grad_reg_loss_type, \n",
    "            grad_reg_loss_params=grad_reg_loss_params, \n",
    "            dummy_input=(344,1,9),\n",
    "            ref_latency=reference_latency,\n",
    "            rubicon=rubicon,\n",
    "            default=default,\n",
    "            num_epochs=epochs,\n",
    "            rub_sched=rub_sched,\n",
    "            dart_sched=dart_sched,\n",
    "            rub_ctrl_opt=rub_ctrl_opt,\n",
    "            prox_ctrl_opt=prox_ctrl_opt               \n",
    "        )\n",
    "\n",
    "trainer.fit(workdir, epochs, lr)\n",
    "final_architecture = trainer.export()\n",
    "_logger.info(\"Final architecture:{}\".format(trainer.export()))\n",
    "\n",
    "# the json file where the output must be stored\n",
    "out_file = open(arc_checkpoint, \"w\")\n",
    "json.dump(final_architecture, out_file, indent = 6)\n",
    "out_file.close()\n",
    "\n",
    "_logger.info(\"JSON file saved at:{}\".format(os.path.expanduser(args.arc_checkpoint)))\n",
    "_logger.info(\"End date and time:{}\".format(datetime.datetime.now()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
